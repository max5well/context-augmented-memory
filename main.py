"""
main.py
Main entry point for the Context-Augmented Memory (CAM) system.
"""

from modules import llm_client, auto_tagger, memory, retrieval, context_decider
from datetime import datetime
from nanoid import generate
import os

# Silence parallel tokenizer warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"


def main():
    print("ğŸ§  Context-Augmented Memory System (CAM)")
    print("Type 'exit' to quit or 'clear memory' to reset stored context.\n")

    while True:
        user_prompt = input("Enter your prompt: ").strip()

        # --- Quit command ---
        if user_prompt.lower() == "exit":
            print("ğŸ‘‹ Goodbye!")
            break

        # --- ğŸ§¹ Memory clear command ---
        if user_prompt.lower() in {"clear memory", "reset memory"}:
            all_ids = memory.collection.get().get("ids", [])
            if all_ids:
                memory.collection.delete(ids=all_ids)
                print(f"ğŸ§¹ Memory cleared ({len(all_ids)} entries removed).\n")
            else:
                print("âš ï¸ No memory entries to clear.\n")
            continue

        # --- ğŸ§  Context continuity check ---
        should_use_context = context_decider.should_retrieve(user_prompt)
        context = ""

        if should_use_context:
            print("ğŸ” Semantic continuity detected â€” retrieving context...")
            context = retrieval.retrieve_context(user_prompt)
        else:
            print("âš™ï¸ New topic detected â€” skipping retrieval.")

        if context:
            print("\nğŸ“š Retrieved context found â€” augmenting your prompt...\n")
            print("ğŸ” Retrieved memory content:\n")
            print("=" * 80)
            print(context)
            print("=" * 80 + "\n")
            full_prompt = f"Context:\n{context}\n\nUser: {user_prompt}"
        else:
            full_prompt = user_prompt

        # --- ğŸ’¬ Send to LLM ---
        print("ğŸ’¬ Sending prompt to LLM...\n")
        llm_output = llm_client.ask(full_prompt)

        # --- ğŸ·ï¸ Tag and store memory ---
        selected_tag = auto_tagger.auto_tag(user_prompt)
        episode_id = generate(size=12)
        metadata = {
            "timestamp": datetime.utcnow().isoformat(),
            "user_prompt": user_prompt,
            "tag": selected_tag,
            # âœ… store as string ("true"/"false") for Chroma compatibility
            "topic_continued": str(should_use_context).lower(),
        }

        memory.collection.add(
            ids=[episode_id],
            documents=[user_prompt],  # store only user input for embeddings
            metadatas=[metadata],
        )


        print(f"âœ… Added episode {episode_id}\n")
        print("ğŸ¤– LLM Output:\n", llm_output)
        print(f"\nğŸ§  Episode {episode_id} stored (tag: {selected_tag}, continued: {should_use_context})")
        print("-" * 60 + "\n")


if __name__ == "__main__":
    main()
